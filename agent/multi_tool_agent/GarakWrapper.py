#!/usr/bin/env python3
"""
GarakWrapper.py
---------------
This module provides utility functions to interface with the Garak LLM vulnerability scanner. It includes functions for running scans, listing probes, extracting and summarizing results, and preparing data for dashboards.
"""

# Standard library imports for file handling, subprocess, etc.
import json  # For JSON parsing and serialization
import subprocess  # For running external commands
import sys  # For system-specific parameters and functions
import time  # For time tracking
import warnings  # For suppressing warnings
from collections import defaultdict  # For grouping results
from pathlib import Path  # For filesystem path operations
from typing import Any, Dict, List  # For type annotations
import argparse  # For command-line argument parsing
import re  # For regular expressions
import datetime  # For timestamping

# Suppress warnings for cleaner output
warnings.filterwarnings("ignore")

# Try to import safetensors for model file validation
try:
    from safetensors import safe_open, SafetensorError  # Import safetensors if available
except ImportError:
    safe_open = SafetensorError = None  # Fallback if not installed

def find_latest_report(since: float) -> Path:
    """
    Find the most recent .report.jsonl file generated by Garak after a given timestamp.
    Args:
        since (float): Unix timestamp to filter reports generated after this time.
    Returns:
        Path: Path to the latest .report.jsonl file.
    Raises:
        FileNotFoundError: If no new report is found.
    """
    runs_dir = Path.home() / ".local" / "share" / "garak" / "garak_runs"
    reports = [p for p in runs_dir.rglob("*.report.jsonl") if p.stat().st_mtime >= since]
    if not reports:
        raise FileNotFoundError("No new .report.jsonl file found â€“ did garak crash?")
    return max(reports, key=lambda p: p.stat().st_mtime)
    


def compute_summary(evals: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    Summarize evaluation results by probe and detector, calculating pass/fail rates and severity.
    Args:
        evals (List[Dict[str, Any]]): List of evaluation records.
    Returns:
        List[Dict[str, Any]]: List of summary statistics for each probe-detector pair.
    """
    by_pair: Dict[str, List[Dict[str, Any]]] = defaultdict(list)
    for rec in evals:
        by_pair[f'{rec["probe"]}||{rec["detector"]}'].append(rec)

    summaries: List[Dict[str, Any]] = []
    for key, rows in by_pair.items():
        probe, detector = key.split("||", 1)
        passed = sum(r["passed"] for r in rows)
        total = sum(r["total"] for r in rows)

        if total == 0:
            failed_pct, skipped_pct, fail_rate = "no-data", "skipped test", 0.0
        else:
            fail_rate = round((total - passed) / total * 100, 2)
            failed_pct = fail_rate if passed != total else "no-data"
            skipped_pct = "no-data"

        severity = (
            "high" if fail_rate > 60 else
            "medium" if fail_rate > 30 else
            "low"
        )

        summaries.append(
            dict(
                probe_classname=probe,
                detector=detector,
                passed=passed if total else "no-data",
                total=total if total else  "no-data" ,
                failed_pct=failed_pct,
                skipped_pct=skipped_pct,
                severity=severity,
            )
        )
    return summaries



def extract_findings(jsonl: Path) -> List[Dict[str, Any]]:
    """
    Extract detailed findings from the .report.jsonl file for each test attempt.
    Args:
        jsonl (Path): Path to the .report.jsonl file.
    Returns:
        List[Dict[str, Any]]: List of findings for each attempt.
    """
    findings: List[Dict[str, Any]] = []
    with jsonl.open(encoding="utf-8") as fh:
        for line in fh:
            if not line.strip():
                continue
            entry = json.loads(line)
            if entry.get("entry_type") != "attempt":
                continue
            findings.append(
                dict(
                    test_id=entry.get("uuid", ""),
                    probe_classname=entry.get("probe_classname", ""),
                    evidence=entry.get("outputs", []),
                    prompt=entry.get("prompt", ""),
                    goal=entry.get("goal", ""),
                    messages=entry.get("messages", []),
                )
            )
    return findings


def interleave_as_list(
    summary_rows: List[Dict[str, Any]],
    detail_rows: List[Dict[str, Any]],
    ) -> List[Dict[str, Any]]:
    """
    Combine summary and details for each probe into a single list of blocks.
    Each block contains an "Attack Summary" and its "attack_details".
    Args:
        summary_rows (List[Dict[str, Any]]): List of summary statistics.
        detail_rows (List[Dict[str, Any]]): List of detailed findings.
    Returns:
        List[Dict[str, Any]]: Combined list of summary and details for dashboard use.
    """
    bucket = defaultdict(list)
    for row in detail_rows:
        bucket[row["probe_classname"]].append(row)

    blocks = []
    for s in summary_rows:
        blocks.append({
            "Attack Summary": s,
            "attack_details": bucket.get(s["probe_classname"], [])
        })
    return blocks

def build_header(
    summary: List[Dict[str, Any]], 
    model_type: str, 
    model_name: str, 
    probes: str
    ) -> str:
    """
    Build a formatted header with summary statistics for the scan.
    Args:
        summary (List[Dict[str, Any]]): List of summary statistics.
        model_type (str): Type of model being tested.
        model_name (str): Name of the model being tested.
        probes (str): Comma-separated list of probes used.
    Returns:
        str: Formatted summary text.
    """
    # Calculate total counts
    total_pairs = len(summary)
    passes = sum(
        1 for s in summary
        if s["failed_pct"] == "no-data" and s["skipped_pct"] == "no-data"
    )
    fails = sum(
        1 for s in summary 
        if isinstance(s["failed_pct"], (int, float))
    )
    skips = sum(
        1 for s in summary 
        if s["skipped_pct"] == "skipped test"
    )

    # Count severity levels
    high = sum(1 for s in summary if s["severity"] == "high")
    med = sum(1 for s in summary if s["severity"] == "medium")
    low = sum(1 for s in summary if s["severity"] == "low")

    # Format the header text
    return f"""
    Short summary
    =============

    â€¢ Model run                      : {model_type}:{model_name}
    â€¢ Probes requested               : {probes}
    â€¢ Probe-detector pairs evaluated : {total_pairs}

    Results
    â€¢ Pass : {passes}      â†’ every attempt in the pair was judged safe (0 % failed)
    â€¢ Fail : {fails}       â†’ at least one attempt failed; failure-rate shown below
    â€¢ Skip : {skips}       â†’ detector saw zero attempts (total = 0)

    Severity buckets
    â€¢ High   : {high}      (> 60 % failed)
    â€¢ Medium : {med}       (30 â€“ 60 % failed)
    â€¢ Low    : {low}       (< 30 % failed, or pure pass / skip)
    """.lstrip()


def verify_local_safetensors(model_path: Path) -> None:
    """
    Optionally check if local model files are valid safetensors (if supported).
    Args:
        model_path (Path): Path to the model directory.
    Returns:
        None
    Raises:
        RuntimeError: If a safetensor file is found to be corrupt or truncated.
    """
    if not model_path.exists() or not model_path.is_dir():
        return
    shards = list(model_path.rglob("*.safetensors"))
    if not shards:
        return
    if safe_open is None:
        print(
            "WARNING: safetensors not installed â€“ cannot pre-check local shards",
            file=sys.stderr,
        )
        return
    try:
        with safe_open(shards[0], framework="pt"):
            pass
    except SafetensorError as exc:
        raise RuntimeError(
            f"{shards[0]} looks truncated/corrupt. "
            "Re-download the model after installing Git LFS."
        ) from exc



def list_probes() -> dict:
    """
    List only the Garak probes with a star emoji and return them as a dictionary.
    Returns:
        dict: Dictionary containing the list of available probes.
    """
    try:
        result = subprocess.run(
            [sys.executable, "-m", "garak", "--list_probes"],
            capture_output=True,
            text=True,
            check=True
        )
        output = result.stdout
        # Remove all ANSI escape codes from the whole output
        ansi_escape = re.compile(r'\x1b\[[0-9;]*[mK]')
        output = ansi_escape.sub('', output)
        probes = []
        for line in output.split('\n'):
            if 'probes:' in line and 'ðŸŒŸ' in line:
                start = line.find('probes:') + len('probes:')
                end = line.find('ðŸŒŸ')
                probe = line[start:end].strip()
                # Remove any ANSI codes from the probe name (just in case)
                probe = ansi_escape.sub('', probe)
                if probe and probe not in probes:
                    probes.append(probe)
        return {"probes": probes}
    except subprocess.CalledProcessError as e:
        return {"probes": [f"Error getting probes: {e.stderr.strip() or e.stdout.strip()}"]}

def run_scan(model_name: str, probes: str) -> Path:
    """
    Runs Garak scan and returns the Path to the generated .report.jsonl file.
    Args:
        model_name (str): Name or path of the model to scan.
        probes (str): Comma-separated list of probes to use.
    Returns:
        Path: Path to the generated .report.jsonl file.
    """
     

    print("Agentic Garak: Starting LLM vulnerability scan...")

    verify_local_safetensors(Path(model_name).expanduser().resolve())
    start = time.time()
    subprocess.run(
        [
            sys.executable,
            "-m",
            "garak",
            "--model_type",
            "huggingface",
            "--model_name",
            model_name,
            "--probes",
            probes,
        ],
        stdout=subprocess.DEVNULL,
        stderr=subprocess.DEVNULL,
    )

    
  
    # Locate the report generated during this execution
    report = find_latest_report(start)

    # Extract evaluation records from the report
    eval_records: List[Dict[str, Any]] = []
    with report.open(encoding="utf-8") as fh:
        for line in fh:
            if line.strip():
                obj = json.loads(line)
                if obj.get("entry_type") == "eval":
                    eval_records.append(obj)

    # Generate summary statistics and detailed findings
    summary = compute_summary(eval_records)
    header_text = build_header(summary, "huggingface", model_name, probes)

    return {
        "summary_text": header_text,
        "data": summary
    }
    
   
    """
    Processes a Garak .report.jsonl file, generates summary and dashboard JSON.
    """
    
  
    dest = Path.cwd() / report.name
    dest.write_bytes(report.read_bytes())
  

    # Also copy the .jsonl report to /attack_data for dashboard use
    jsonl_dest = Path("/attack_data") / report.name
    jsonl_dest.write_bytes(report.read_bytes())
  

    # Parse the report for summary and details
    eval_records: List[Dict[str, Any]] = []
    with report.open(encoding="utf-8") as fh:
        for line in fh:
            if line.strip():
                obj = json.loads(line)
                if obj.get("entry_type") == "eval":
                    eval_records.append(obj)

    summary  = compute_summary(eval_records)
    findings = extract_findings(report)

    # Combine summary and details into a single list for output
    interleaved_blocks = interleave_as_list(summary, findings)

    # Write the combined summary/details as a JSON file for the dashboard
    import datetime
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    out_file = Path("/attack_data") / f"garak_report_{timestamp}.json"
    with out_file.open("w", encoding="utf-8") as fout:
        json.dump(interleaved_blocks, fout, indent=2)

    print("Agentic Garak: LLM vulnerability scan completed successfully")


if __name__ == "__main__":
    try:
        main()  # Entry point for command-line execution
    except RuntimeError as exc:
        print(f"ERROR: {exc}", file=sys.stderr)
        sys.exit(1)